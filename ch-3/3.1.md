**3.1-1:**

Let f(n) and g(n) be asymptotically nonnegative functions. Using the basic definition of Theta Notation, prove that max(f(n), g(n)) = THETA(f(n) + g(n))

THETA(g(n)) represents a set of functions such that there exists positive constants c1, c2, & n0 such that `0 <= c1 * g(n) <= f(n) <= c2 * g(n)` for all n >= n0.

Since f(n) is bound between those two constants, and g(n) times the constants c1 & c2 represents the worst case & best cast run times, if we were to chose the maximum between the two, we'd actually be applying specific values for c1 and c2 that would satisfy the max function:

```
f(n) <= max(f(n), g(n))
g(n) <= max(f(n), g(n))
(f(n) + g(n)) / 2 <= max(f(n), g(n))
0 <= (f(n) + g(n) / 2) <= max(f(n), g(n)) <= f(n) + g(n)
```

In the case above, the constants c1 = 1/2 and c2 = 1

**3.1-2:** Show that for any real constants a and b, where b > 0: (n + a)<sup>b</sup> = THETA(n<sup>b</sup>)

If we were to take the root of b on each side, we would come to (n + a) = THETA(n). Another interpretation of this is that b is the constant of 1. We can then pick any constant for a such that it satisfies the relationship of `0 <= a * g(n) <= f(n) <= b * g(n)`.

**3.1-3:** Explain why the statement, "The running time of algorithm A is at leastO(n<sup>2</sup>)" is meaningless

This is because Big O notation represents the upper bound of a function such that 0 <= f(n) <= c<sub>1</sub> * g(n) for all n > n<sub>0</sub> -- i.e. there is no information in this statement about the lower bounds of algorithm A.

**3.1-4:** Is 2<sup>(n+1)</sup> = O(n<sup>2</sup>)? Is 2<sup>2n</sup> = O(2<sup>n</sup>)?

The first statement is true. We can assign values c1 => 2 and n0 = 0 such that `0 <= 2<sup>(n+1)</sup> <= c1 * 2<sup>n</sup>`

The second statement is false. 2<sup>2n</sup> is equal to 2<sup>n</sup> * 2<sup>n</sup> =  4<sup>n</sup>. There are no constants c1 and n0 which can satisfy that.

**3.1-5:** Prove Theorem 3.1: For any two function f(n) and g(n), we have f(n) = THETA(g(n)) if and only if f(n) = O(g(n)) and f(n) = OMEGA(g(n))

For f(n) = THETA(g(n)), we expand the definition as we have previously for all values of n greater than n<sub>0</sub>:

0 <= c<sub>1</sub> * g(n) <= f(n) <= c<sub>2</sub> * g(n)

We can then blog in values for the constants to satisfy the Big O and OMEGA notation functions:

Omega: 0 <= c<sub>3</sub> * g(n) <= f(n)
Big O: 0 <= f(n) <= c<sub>4</sub> * g(n)

If we merge the inequalities above, we get: 0 <= c<sub>3</sub> * g(n) <= f(n) <= c<sub>4</sub> * g(n), which is the definition of THETA(f(n)).

**3.1-6:** Prove that the running time of an algorithm is THETA(g(n)) if and only if its worst case running time is O(g(n)) and its best case running time is OMEGA(g(n)).

This question is similar to the one above. If T<sub>w</sub> is the worst case run time, and T<sub>b</sub> is the best case run time, we have:

0 <= c<sub>1</sub> * g(n) <= T<sub>b</sub>(n), which means that there is some constant factor applied to the function g(n) such that it runs less than the best case run time

We also have 0 <= T<sub>w</sub>(n) <= c<sub>2</sub> * g(n), which means that there is some constant factor applied to the function g(n) such that it runs faster than the worst case run time.

Both of these functions are realized by different amounts of input (n), but we can combine them to form the original definition:

0 <= c<sub>1</sub> * g(n) <= T<sub>b</sub>(n) <= T<sub>w</sub>(n) <= c<sub>2</sub> * g(n) for n > max(n<sub>b</sub>, n<sub>w</sub>)

**3.1-7:** Prove that Little O(g(n)) INTERSECTION Little Omega(g(n)) is an empty set.

Both Little O and Little Omega represent opposite things:

Little O represents a function f(n) in which all constants * g(n) will be less than the upper bound of g(n) after some sufficient level of n (n<sub>0</sub>). In other words, f(n) has a smaller rate of growth than g(n), and the limit of the ratio f(n) / g(n) will go to 0 as n reaches infinity.

Little Omega represents a function f(n) in which all constants * g(n) will be greater than the lower bound of g(n) after some sufficiently large level of n (n<sub>0</sub>). In other words, f(n) has a larger rate of growth than g(n), and the limit of the ratio f(n) / g(n) will go to infinity as n reaches infinity.

Therefore they have no intersection.

**3.1-8:** We can extend our notation to the case of to parameters n, m that can go to infinity independently at different rates. For a given function g(n, m), we denote by O(g(n,m)) the set of functions:

O(g(n,m)) = { f(n,m) : There exists positive constants c, n<sub>0</sub>, and m<sub>0</sub> such that 0 <= f(n,m) <= cg(n,m) for all n > n<sub>0</sub> or m >= m<sub>0</sub> }.

Give the corresponding definitions for OMEGA(g(n,m)) and THETA(g(n,m))

THETA(g(n,m)): 0 <= c<sub>1</sub> * g(n, m) <= f(n, m) <= c<sub>2</sub> * g(n, m) for all n > n<sub>0</sub> and m > m<sub>0</sub>
OMEGA(g(n,m)): 0 <= c<sub>1</sub> * g(n, m) <= f(n, m) for all n > n<sub>0</sub> and m > m<sub>0</sub>
