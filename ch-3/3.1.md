**3.1-1:**

> Let f(n) and g(n) be asymptotically nonnegative functions. Using the basic definition of &Theta; Notation, prove that max(f(n), g(n)) = &Theta;(f(n) + g(n))

&Theta;(g(n)) represents a set of functions such that there exists positive constants c<sub>1</sub>, c<sub>2</sub>, & n<sub>0</sub> such that 0 <= c<sub>1</sub> * g(n) <= f(n) <= c<sub>2</sub> * g(n) for all n >= n<sub>0</sub>.

Since f(n) is bound between those two constants, and g(n) times the constants c<sub>1</sub> & c<sub>2</sub> represents the worst case & best cast run times, if we were to chose the maximum between the two, we'd actually be applying specific values for c<sub>1</sub> and c<sub>2</sub> that would satisfy the max function:

```
f(n) <= max(f(n), g(n))
g(n) <= max(f(n), g(n))
(f(n) + g(n)) / 2 <= max(f(n), g(n))
0 <= (f(n) + g(n) / 2) <= max(f(n), g(n)) <= f(n) + g(n)
```

In the case above, the constants c<sub>1</sub> = 1/2 and c<sub>2</sub> = 1

**3.1-2:**

> Show that for any real constants a and b, where b > 0: (n + a)<sup>b</sup> = &Theta;(n<sup>b</sup>)

If we were to take the root of b on each side, we would come to (n + a) = &Theta;(n). Another interpretation of this is that b is the constant of 1. We can then pick any constant for a such that it satisfies the relationship of `0 <= a * g(n) <= f(n) <= b * g(n)`.

**3.1-3:**

> Explain why the statement, "The running time of algorithm A is at least O(n<sup>2</sup>)" is meaningless

This is because &Omicron; notation represents the upper bound of a function such that 0 <= f(n) <= c<sub>1</sub> * g(n) for all n > n<sub>0</sub> -- i.e. there is no information in this statement about the lower bounds of algorithm A.

**3.1-4:**

> Is 2<sup>(n+1)</sup> = O(n<sup>2</sup>)? Is 2<sup>2n</sup> = O(2<sup>n</sup>)?

The first statement is true. We can assign values c<sub>1</sub> => 2 and n<sub>0</sub> = 0 such that 0 <= 2<sup>(n+1)</sup> <= c<sub>1</sub> * 2<sup>n</sup>

The second statement is false. 2<sup>2n</sup> is equal to 2<sup>n</sup> * 2<sup>n</sup> =  4<sup>n</sup>. There are no constants c<sub>1</sub> and n<sub>0</sub> which can satisfy that.

**3.1-5:**

> Prove Theorem 3.1: For any two function f(n) and g(n), we have f(n) = &Theta;(g(n)) if and only if f(n) = O(g(n)) and f(n) = &Omega;(g(n))

For f(n) = &Theta;(g(n)), we expand the definition as we have previously for all values of n greater than n<sub>0</sub>:

0 <= c<sub>1</sub> * g(n) <= f(n) <= c<sub>2</sub> * g(n)

We can then blog in values for the constants to satisfy the &Omicron; and &Omega; notation functions:

&Omega;: 0 <= c<sub>3</sub> * g(n) <= f(n)

&Omicron;: 0 <= f(n) <= c<sub>4</sub> * g(n)

If we merge the inequalities above, we get: 0 <= c<sub>3</sub> * g(n) <= f(n) <= c<sub>4</sub> * g(n), which is the definition of &Theta;(f(n)).

**3.1-6:**

> Prove that the running time of an algorithm is &Theta;(g(n)) if and only if its worst case running time is &Omicron;(g(n)) and its best case running time is &Omega;(g(n)).

This question is similar to the one above. If T<sub>w</sub> is the worst case run time, and T<sub>b</sub> is the best case run time, we have:

0 <= c<sub>1</sub> * g(n) <= T<sub>b</sub>(n), which means that there is some constant factor applied to the function g(n) such that it runs less than the best case run time

We also have 0 <= T<sub>w</sub>(n) <= c<sub>2</sub> * g(n), which means that there is some constant factor applied to the function g(n) such that it runs faster than the worst case run time.

Both of these functions are realized by different amounts of input (n), but we can combine them to form the original definition:

0 <= c<sub>1</sub> * g(n) <= T<sub>b</sub>(n) <= T<sub>w</sub>(n) <= c<sub>2</sub> * g(n) for n > max(n<sub>b</sub>, n<sub>w</sub>)

**3.1-7:**

> Prove that &omicron;(g(n)) &Pi; &omega;(g(n)) is an empty set.

Both &omicron; and &omega; represent opposite things:

&omicron; represents a function f(n) in which all constants * g(n) will be less than the upper bound of g(n) after some sufficient level of n (n<sub>0</sub>). In other words, f(n) has a smaller rate of growth than g(n), and the limit of the ratio f(n) / g(n) will go to 0 as n reaches infinity.

&omega; represents a function f(n) in which all constants * g(n) will be greater than the lower bound of g(n) after some sufficiently large level of n (n<sub>0</sub>). In other words, f(n) has a larger rate of growth than g(n), and the limit of the ratio f(n) / g(n) will go to infinity as n reaches infinity.

Therefore they have no intersection.

**3.1-8:**

> We can extend our notation to the case of to parameters n, m that can go to infinity independently at different rates. For a given function g(n, m), we denote by &Omicron;(g(n,m)) the set of functions:

&Omicron;(g(n,m)) = { f(n,m) : There exists positive constants c, n<sub>0</sub>, and m<sub>0</sub> such that 0 <= f(n,m) <= c * g(n,m) for all n > n<sub>0</sub> or m >= m<sub>0</sub> }.

Give the corresponding definitions for &Omega;(g(n,m)) and &Theta;(g(n,m))

&Theta;(g(n,m)): 0 <= c<sub>1</sub> * g(n, m) <= f(n, m) <= c<sub>2</sub> * g(n, m) for all n > n<sub>0</sub> and m > m<sub>0</sub>

&Omega;(g(n,m)): 0 <= c<sub>1</sub> * g(n, m) <= f(n, m) for all n > n<sub>0</sub> and m > m<sub>0</sub>
