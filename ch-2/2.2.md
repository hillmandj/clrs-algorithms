**2.2-1:**

*Express the function n^3/1000 - 100n^2 - 100n + 3 in terms of Big O Notation (theta notation)*

Since we discard lower level terms when doing Order of Growth analysis on algorithms, we can say that this function is O(n^3)

**2.2-2:**

*Consider sorting n numbers stored in array A by first finding the smallest element of A and exchanging it with the element in A[1].
Then find the second smallest element of A, and exchange it with A[2]. Continue in this manner for the first n-1 elements of A. Write
pseudocode for this algorithm, which is known as "Selection Sort". What loop invariant does this algorithm maintain? Why does it need
to run for only the first n-1 elements, rather than for all n elements? Give the best case and worst case running times of selection
sort in Big O notation. Justify your answers.*

```ruby
# Iterate up to pentultimate item
0.upto(A.length - 1) do |j|
  # Get index of current and next item, initialize current item as the minimum
  min = j
  i = j + 1

  # Go through the rest of the list, and find smallest element
  # Terminate when we've run out of items to check
  while i < A.length
    # If the current element is less than that of smallest, replace it
    if A[i] < A[min]
      min = i
    end
    i += 1
  end

  # Swap the initial key with the smallest element
  a[j], a[min] = a[min], a[j]
end
```

See full implementation [here](#)

The loop invariant that this algorithm maintains is that with each iteration of the loop we ensure that the minimal element relative to the one at the current index swap places. In other words, the subarray A[0..j] is always sorted.

Using "Worst Case Analysis" this algorithm is O(n^2). This is because if the list were already sorted in descending order, for each iteration we would have to cycle through and check every index before making our final swap.
